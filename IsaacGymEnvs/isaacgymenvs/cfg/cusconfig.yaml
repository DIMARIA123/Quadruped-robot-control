# Task name - used to pick the class to load
task_name: ${task.name}
# experiment name. defaults to name of training config
experiment: '1'

# chose the env name
env_name: 'rlgpu'

# if set to positive integer, overrides the default number of environments
num_envs: 512

# seed - set to -1 to choose random seed
seed: 42

# set some training parameters
max_iterations: 1000
save_best_after: 200
save_frequency: 50
score_to_win: 20000

## Device config
#  'physx' or 'flex'
physics_engine: 'physx'
# whether to use cpu or gpu pipeline
pipeline: 'gpu'
# device for running physics simulation
sim_device: 'cuda:0'
# device to run RL
rl_device: 'cuda:0'
graphics_device_id: 0

# RLGames Arguments
# test - if set, run policy in inference mode (requires setting checkpoint to load)
test: False
# used to set checkpoint path
checkpoint: ''
# Set the standard deviation for exploration noise; leave empty to disable noise
sigma: ''
# set to True to use multi-gpu training
multi_gpu: False

capture_video: False
capture_video_freq: 1464
capture_video_len: 100
force_render: True

# disables rendering
headless: False

# Multiple Configuration Files Composition
defaults:
  - task: Go2
  - train: ${task}PPO
  - override hydra/job_logging: disabled # disable hydra logging
  - _self_

# set the directory where the output files get saved(disabled by default)
hydra:
  output_subdir: null
  run:
    dir: .

